{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FaceJ:\n",
    "    a simplified YOLO-based face detection model trained with WIDER_FACE datasets;\n",
    "\n",
    "YOLO training pipeline:\n",
    "    adapted from MXNET/GLUON tutorial;\n",
    "\n",
    "WIDER_FACE:\n",
    "    converted from jpg to rec using gluon tool im2rec;\n",
    "\n",
    "Architecture:\n",
    "    1. Resnet18: high speed;\n",
    "    2. Resnet34: high accuracy;\n",
    "    3. originally designed; ideas adopted from https://towardsdatascience.com/faced-cpu-real-time-face-detection-using-deep-learning-1488681c1602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "\n",
    "from mxboard import SummaryWriter\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon import Block, HybridBlock\n",
    "from mxnet.gluon.model_zoo import vision\n",
    "from mxnet import metric\n",
    "from mxnet import init\n",
    "from mxnet import gpu\n",
    "from mxnet import autograd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_center(xy):\n",
    "    \"\"\"Given x, y prediction after sigmoid(), convert to relative coordinates (0, 1) on image.\"\"\"\n",
    "    b, h, w, n, s = xy.shape\n",
    "    offset_y = nd.tile(nd.arange(0, h, repeat=(w * n * 1), ctx=xy.context).reshape((1, h, w, n, 1)), (b, 1, 1, 1, 1))\n",
    "    # print(offset_y[0].asnumpy()[:, :, 0, 0])\n",
    "    offset_x = nd.tile(nd.arange(0, w, repeat=(n * 1), ctx=xy.context).reshape((1, 1, w, n, 1)), (b, h, 1, 1, 1))\n",
    "    # print(offset_x[0].asnumpy()[:, :, 0, 0])\n",
    "    x, y = xy.split(num_outputs=2, axis=-1)\n",
    "    x = (x + offset_x) / w\n",
    "    y = (y + offset_y) / h\n",
    "    return x, y\n",
    "\n",
    "def transform_size(wh, anchors):\n",
    "    \"\"\"Given w, h prediction after exp() and anchor sizes, convert to relative width/height (0, 1) on image\"\"\"\n",
    "    b, h, w, n, s = wh.shape\n",
    "    aw, ah = nd.tile(nd.array(anchors, ctx=wh.context).reshape((1, 1, 1, -1, 2)), (b, h, w, 1, 1)).split(num_outputs=2, axis=-1)\n",
    "    w_pred, h_pred = nd.exp(wh).split(num_outputs=2, axis=-1)\n",
    "    w_out = w_pred * aw / w\n",
    "    h_out = h_pred * ah / h\n",
    "    return w_out, h_out\n",
    "\n",
    "def yolo2_forward(x, num_class, anchor_scales):\n",
    "    \"\"\"Transpose/reshape/organize convolution outputs.\"\"\"\n",
    "    stride = num_class + 5\n",
    "    # transpose and reshape, 4th dim is the number of anchors\n",
    "    x = x.transpose((0, 2, 3, 1))\n",
    "    x = x.reshape((0, 0, 0, -1, stride))\n",
    "    # now x is (batch, m, n, stride), stride = num_class + 1(object score) + 4(coordinates)\n",
    "    # class probs\n",
    "    cls_pred = x.slice_axis(begin=0, end=num_class, axis=-1)\n",
    "    # object score\n",
    "    score_pred = x.slice_axis(begin=num_class, end=num_class + 1, axis=-1)\n",
    "    score = nd.sigmoid(score_pred)\n",
    "    # center prediction, in range(0, 1) for each grid\n",
    "    xy_pred = x.slice_axis(begin=num_class + 1, end=num_class + 3, axis=-1)\n",
    "    xy = nd.sigmoid(xy_pred)\n",
    "    # width/height prediction\n",
    "    wh = x.slice_axis(begin=num_class + 3, end=num_class + 5, axis=-1)\n",
    "    # convert x, y to positions relative to image\n",
    "    x, y = transform_center(xy)\n",
    "    # convert w, h to width/height relative to image\n",
    "    w, h = transform_size(wh, anchor_scales)\n",
    "    # cid is the argmax channel\n",
    "    cid = nd.argmax(cls_pred, axis=-1, keepdims=True)\n",
    "    # convert to corner format boxes\n",
    "    half_w = w / 2\n",
    "    half_h = h / 2\n",
    "    left = nd.clip(x - half_w, 0, 1)\n",
    "    top = nd.clip(y - half_h, 0, 1)\n",
    "    right = nd.clip(x + half_w, 0, 1)\n",
    "    bottom = nd.clip(y + half_h, 0, 1)\n",
    "    output = nd.concat(*[cid, score, left, top, right, bottom], dim=4)\n",
    "    return output, cls_pred, score, nd.concat(*[xy, wh], dim=4)\n",
    "\n",
    "def corner2center(boxes, concat=True):\n",
    "    \"\"\"Convert left/top/right/bottom style boxes into x/y/w/h format\"\"\"\n",
    "    left, top, right, bottom = boxes.split(axis=-1, num_outputs=4)\n",
    "    x = (left + right) / 2\n",
    "    y = (top + bottom) / 2\n",
    "    width = right - left\n",
    "    height = bottom - top\n",
    "    if concat:\n",
    "        last_dim = len(x.shape) - 1\n",
    "        return nd.concat(*[x, y, width, height], dim=last_dim)\n",
    "    return x, y, width, height\n",
    "\n",
    "def center2corner(boxes, concat=True):\n",
    "    \"\"\"Convert x/y/w/h style boxes into left/top/right/bottom format\"\"\"\n",
    "    x, y, w, h = boxes.split(axis=-1, num_outputs=4)\n",
    "    w2 = w / 2\n",
    "    h2 = h / 2\n",
    "    left = x - w2\n",
    "    top = y - h2\n",
    "    right = x + w2\n",
    "    bottom = y + h2\n",
    "    if concat:\n",
    "        last_dim = len(left.shape) - 1\n",
    "        return nd.concat(*[left, top, right, bottom], dim=last_dim)\n",
    "    return left, top, right, bottom\n",
    "\n",
    "def yolo2_target(scores, boxes, labels, anchors, ignore_label=-1, thresh=0.5):\n",
    "    \"\"\"Generate training targets given predictions and labels.\"\"\"\n",
    "    b, h, w, n, _ = scores.shape\n",
    "    anchors = np.reshape(np.array(anchors), (-1, 2))\n",
    "    #scores = nd.slice_axis(outputs, begin=1, end=2, axis=-1)\n",
    "    #boxes = nd.slice_axis(outputs, begin=2, end=6, axis=-1)\n",
    "    gt_boxes = nd.slice_axis(labels, begin=1, end=5, axis=-1)\n",
    "    target_score = nd.zeros((b, h, w, n, 1), ctx=scores.context)\n",
    "    target_id = nd.ones_like(target_score, ctx=scores.context) * ignore_label\n",
    "    target_box = nd.zeros((b, h, w, n, 4), ctx=scores.context)\n",
    "    sample_weight = nd.zeros((b, h, w, n, 1), ctx=scores.context)\n",
    "    for b in range(output.shape[0]):\n",
    "        # find the best match for each ground-truth\n",
    "        label = labels[b].asnumpy()\n",
    "        valid_label = label[np.where(label[:, 0] > -0.5)[0], :]\n",
    "        # shuffle because multi gt could possibly match to one anchor, we keep the last match randomly\n",
    "        np.random.shuffle(valid_label)\n",
    "        for l in valid_label:\n",
    "            gx, gy, gw, gh = (l[1] + l[3]) / 2, (l[2] + l[4]) / 2, l[3] - l[1], l[4] - l[2]\n",
    "            ind_x = int(gx * w)\n",
    "            ind_y = int(gy * h)\n",
    "            tx = gx * w - ind_x\n",
    "            ty = gy * h - ind_y\n",
    "            gw = gw * w\n",
    "            gh = gh * h\n",
    "            # find the best match using width and height only, assuming centers are identical\n",
    "            intersect = np.minimum(anchors[:, 0], gw) * np.minimum(anchors[:, 1], gh)\n",
    "            ovps = intersect / (gw * gh + anchors[:, 0] * anchors[:, 1] - intersect)\n",
    "            best_match = int(np.argmax(ovps))\n",
    "            target_id[b, ind_y, ind_x, best_match, :] = l[0]\n",
    "            target_score[b, ind_y, ind_x, best_match, :] = 1.0\n",
    "            tw = np.log(gw / anchors[best_match, 0])\n",
    "            th = np.log(gh / anchors[best_match, 1])\n",
    "            target_box[b, ind_y, ind_x, best_match, :] = mx.nd.array([tx, ty, tw, th])\n",
    "            sample_weight[b, ind_y, ind_x, best_match, :] = 1.0\n",
    "            # print('ind_y', ind_y, 'ind_x', ind_x, 'best_match', best_match, 't', tx, ty, tw, th, 'ovp', ovps[best_match], 'gt', gx, gy, gw/w, gh/h, 'anchor', anchors[best_match, 0], anchors[best_match, 1])\n",
    "    return target_id, target_score, target_box, sample_weight\n",
    "\n",
    "class YOLO2Output(HybridBlock):\n",
    "    def __init__(self, num_class, anchor_scales, **kwargs):\n",
    "        super(YOLO2Output, self).__init__(**kwargs)\n",
    "        assert num_class > 0, \"number of classes should > 0, given {}\".format(num_class)\n",
    "        self._num_class = num_class\n",
    "        assert isinstance(anchor_scales, (list, tuple)), \"list or tuple of anchor scales required\"\n",
    "        assert len(anchor_scales) > 0, \"at least one anchor scale required\"\n",
    "        for anchor in anchor_scales:\n",
    "            assert len(anchor) == 2, \"expected each anchor scale to be (width, height), provided {}\".format(anchor)\n",
    "        self._anchor_scales = anchor_scales\n",
    "        out_channels = len(anchor_scales) * (num_class + 1 + 4)\n",
    "        with self.name_scope():\n",
    "            self.output = nn.Conv2D(out_channels, 1, 1)\n",
    "\n",
    "    def hybrid_forward(self, F, x, *args):\n",
    "        return self.output(x)\n",
    "\n",
    "def get_iterators(data_shape, batch_size):\n",
    "    class_names = ['face', 'dummy']\n",
    "    num_class = len(class_names)\n",
    "    train_iter = image.ImageDetIter(\n",
    "        batch_size=batch_size,\n",
    "        data_shape=(3, data_shape, data_shape),\n",
    "        path_imgrec=data_dir+'wider_train.rec',\n",
    "        path_imgidx=data_dir+'wider_train.idx',\n",
    "        shuffle=True,\n",
    "        mean=True,\n",
    "        std=True,\n",
    "        rand_crop=1,\n",
    "        min_object_covered=0.95,\n",
    "        max_attempts=200)\n",
    "    val_iter = image.ImageDetIter(\n",
    "        batch_size=batch_size,\n",
    "        data_shape=(3, data_shape, data_shape),\n",
    "        path_imgrec=data_dir+'wider_val.rec',\n",
    "        shuffle=False,\n",
    "        mean=True,\n",
    "        std=True)\n",
    "    return train_iter, val_iter, class_names, num_class\n",
    "\n",
    "class LossRecorder(mx.metric.EvalMetric):\n",
    "    \"\"\"LossRecorder is used to record raw loss so we can observe loss directly\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        super(LossRecorder, self).__init__(name)\n",
    "\n",
    "    def update(self, labels, preds=0):\n",
    "        \"\"\"Update metric with pure loss\n",
    "        \"\"\"\n",
    "        for loss in labels:\n",
    "            if isinstance(loss, mx.nd.NDArray):\n",
    "                loss = loss.asnumpy()\n",
    "            self.sum_metric += loss.sum()\n",
    "            self.num_inst += 1\n",
    "\n",
    "def process_image(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        im = image.imdecode(f.read())\n",
    "    # resize to data_shape\n",
    "    data = image.imresize(im, data_shape, data_shape)\n",
    "    # minus rgb mean, divide std\n",
    "    data = (data.astype('float32') - rgb_mean) / rgb_std\n",
    "    # convert to batch x channel x height xwidth\n",
    "    return data.transpose((2,0,1)).expand_dims(axis=0), im\n",
    "\n",
    "def predict(x):\n",
    "    x = net(x)\n",
    "    output, cls_prob, score, xywh = yolo2_forward(x, 2, scales)\n",
    "    return nd.contrib.box_nms(output.reshape((0, -1, 6)))\n",
    "\n",
    "def display(im, out, threshold=0.5):\n",
    "    plt.imshow(im.asnumpy())\n",
    "    for row in out:\n",
    "        row = row.asnumpy()\n",
    "        class_id, score = int(row[0]), row[1]\n",
    "        if class_id < 0 or score < threshold:\n",
    "            continue\n",
    "        color = colors[class_id%len(colors)]\n",
    "        box = row[2:6] * np.array([im.shape[1],im.shape[0]]*2)\n",
    "        rect = box_to_rect(nd.array(box), color, 2)\n",
    "        plt.gca().add_patch(rect)\n",
    "        text = class_names[class_id]\n",
    "        plt.gca().text(box[0], box[1],\n",
    "                       '{:s} {:.2f}'.format(text, score),\n",
    "                       bbox=dict(facecolor=color, alpha=0.5),\n",
    "                       fontsize=10, color='white')\n",
    "        plt.savefig('1image_mulifaces_bx.png')\n",
    "    plt.show()\n",
    "    \n",
    "mpl.rcParams['figure.dpi']= 120\n",
    "def box_to_rect(box, color, linewidth=3):\n",
    "    \"\"\"convert an anchor box to a matplotlib rectangle\"\"\"\n",
    "    box = box.asnumpy()\n",
    "    return plt.Rectangle(\n",
    "        (box[0], box[1]), box[2]-box[0], box[3]-box[1],\n",
    "        fill=False, edgecolor=color, linewidth=linewidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIDER_FACE directory\n",
    "data_dir = '/data/face_detector/wider_voc/train/'\n",
    "\n",
    "# ImageDetIter WIDER_FACE\n",
    "data_shape = 256\n",
    "batch_size = 16\n",
    "rgb_mean = nd.array([123, 117, 104])\n",
    "rgb_std = nd.array([58.395, 57.12, 57.375])\n",
    "\n",
    "train_data, test_data, class_names, num_class = get_iterators(\n",
    "    data_shape, batch_size)\n",
    "\n",
    "batch = train_data.next()\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_, figs = plt.subplots(3, 3, figsize=(6,6))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        img, labels = batch.data[0][3*i+j], batch.label[0][3*i+j]\n",
    "        img = img.transpose((1, 2, 0)) * rgb_std + rgb_mean\n",
    "        img = img.clip(0,255).asnumpy()/255\n",
    "        fig = figs[i][j]\n",
    "        fig.imshow(img)\n",
    "        for label in labels:\n",
    "            rect = box_to_rect(label[1:5]*data_shape,'red',2)\n",
    "            fig.add_patch(rect)\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sce_loss = gluon.loss.SoftmaxCrossEntropyLoss(from_logits=False)\n",
    "l1_loss = gluon.loss.L1Loss()\n",
    "\n",
    "obj_loss = LossRecorder('objectness_loss')\n",
    "cls_loss = LossRecorder('classification_loss')\n",
    "box_loss = LossRecorder('box_refine_loss')\n",
    "\n",
    "positive_weight = 5.0\n",
    "negative_weight = 0.1\n",
    "class_weight = 1.0\n",
    "box_weight = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_ori = nn.HybridSequential()\n",
    "class Residual(nn.HybridBlock):\n",
    "    def __init__(self, channels, same_shape=True, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.same_shape = same_shape\n",
    "        with self.name_scope():\n",
    "            strides = 1 if same_shape else 2\n",
    "            self.conv1 = nn.Conv2D(channels, kernel_size=3, padding=1,\n",
    "                                  strides=strides)\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv2 = nn.Conv2D(channels, kernel_size=3, padding=1)\n",
    "            self.bn2 = nn.BatchNorm()\n",
    "            if not same_shape:\n",
    "                self.conv3 = nn.Conv2D(channels, kernel_size=1,\n",
    "                                      strides=strides)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if not self.same_shape:\n",
    "            x = self.conv3(x)\n",
    "        return F.relu(out + x)\n",
    "\n",
    "conv1 = nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1)\n",
    "bn1 = nn.BatchNorm()\n",
    "relu = nn.Activation(activation='relu')\n",
    "maxpool1 = nn.MaxPool2D()\n",
    "net_ori.add(conv1)\n",
    "net_ori.add(bn1)\n",
    "net_ori.add(relu)\n",
    "net_ori.add(maxpool1)\n",
    "for _ in range(3):\n",
    "    net_ori.add(Residual(channels=32))\n",
    "net_ori.add(Residual(channels=64, same_shape=False))\n",
    "for _ in range(2):\n",
    "    net_ori.add(Residual(channels=64))\n",
    "\n",
    "# anchor scales, try adjust it yourself\n",
    "scales = [[3.3004, 3.59034],\n",
    "          [9.84923, 8.23783]]\n",
    "\n",
    "# use 2 classes, 1 as dummy class, otherwise softmax won't work\n",
    "predictor = YOLO2Output(2, scales)\n",
    "predictor.initialize()\n",
    "net_ori.add(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = vision.get_model('resnet18_v1', pretrained=True).features\n",
    "net = nn.HybridSequential()\n",
    "for i in range(len(pretrained) - 2):\n",
    "    net.add(pretrained[i])\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = vision.get_model('resnet18_v1', pretrained=True).features\n",
    "net = nn.HybridSequential()\n",
    "for i in range(len(pretrained) - 2):\n",
    "    net.add(pretrained[i])\n",
    "\n",
    "# anchor scales, try adjust it yourself\n",
    "scales = [[3.3004, 3.59034],\n",
    "          [9.84923, 8.23783]]\n",
    "\n",
    "# use 2 classes, 1 as dummy class, otherwise softmax won't work\n",
    "predictor = YOLO2Output(2, scales)\n",
    "predictor.initialize()\n",
    "net.add(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "lr_period = 50\n",
    "lr_decay = 0.1\n",
    "ctx = mx.cpu()\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        'sgd', {'learning_rate': 1, 'wd': 5e-4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch = 0\n",
    "train_data.reset()\n",
    "cls_loss.reset()\n",
    "obj_loss.reset()\n",
    "box_loss.reset()\n",
    "tic = time.time()\n",
    "for i, batch in enumerate(train_data):\n",
    "    x = batch.data[0].as_in_context(ctx)\n",
    "    y = batch.label[0].as_in_context(ctx)\n",
    "    #print('batch x ==>', x.shape)\n",
    "    #print('batch y ==>', y)\n",
    "    with autograd.record():\n",
    "        x = net(x)\n",
    "        #print(x)\n",
    "        output, cls_pred, score, xywh = yolo2_forward(x, 2, scales)\n",
    "        #print(xywh)\n",
    "        with autograd.pause():\n",
    "            tid, tscore, tbox, sample_weight = yolo2_target(score, xywh, y, scales, thresh=0.5)\n",
    "        #print(class_weight)\n",
    "        # losses\n",
    "        loss1 = sce_loss(cls_pred, tid, sample_weight * class_weight)\n",
    "        #print('loss1 predicted, truth ==>', loss1)\n",
    "        score_weight = nd.where(sample_weight > 0,\n",
    "                                    nd.ones_like(sample_weight) * positive_weight,\n",
    "                                    nd.ones_like(sample_weight) * negative_weight)\n",
    "        loss2 = l1_loss(score, tscore, score_weight)\n",
    "        #print('loss2 predicted, truth ==>', score - tscore)\n",
    "        loss3 = l1_loss(xywh, tbox, sample_weight * box_weight)\n",
    "        print('loss3 predicted ==>', xywh)\n",
    "        print('loss3 truth ==>', tbox)\n",
    "        loss = loss1 + loss2 + loss3\n",
    "    loss.backward()\n",
    "    trainer.step(batch_size)\n",
    "    # update metrics\n",
    "    cls_loss.update(loss1)\n",
    "    obj_loss.update(loss2)\n",
    "    box_loss.update(loss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.hybridize()\n",
    "unique_id = str(datetime.datetime.now()).replace(' ', '-').replace(':', '-').replace('.','-')\n",
    "with SummaryWriter(logdir='/data/face_detector/logs_gluon/logs-'+unique_id, flush_secs=5) as sw:\n",
    "    try:\n",
    "        global_step = 0\n",
    "        for epoch in range(1):\n",
    "            # reset data iterators and metrics\n",
    "            thefile = open('/data/face_detector/logs_gluon/logs-'+unique_id+'/faceJ'+'-'+str(epoch)+'.txt', 'w')\n",
    "            if epoch > 0 and epoch % lr_period == 0:\n",
    "                trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            train_data.reset()\n",
    "            cls_loss.reset()\n",
    "            obj_loss.reset()\n",
    "            box_loss.reset()\n",
    "            tic = time.time()\n",
    "            for i, batch in enumerate(train_data):\n",
    "                global_step += 1\n",
    "                x = batch.data[0].as_in_context(ctx)\n",
    "                y = batch.label[0].as_in_context(ctx)\n",
    "                with autograd.record():\n",
    "                    x = net(x)\n",
    "                    output, cls_pred, score, xywh = yolo2_forward(x, 2, scales)\n",
    "                    with autograd.pause():\n",
    "                        tid, tscore, tbox, sample_weight = yolo2_target(score, xywh, y, scales, thresh=0.5)\n",
    "                    # losses\n",
    "                    loss1 = sce_loss(cls_pred, tid, sample_weight * class_weight)\n",
    "                    score_weight = nd.where(sample_weight > 0,\n",
    "                                            nd.ones_like(sample_weight) * positive_weight,\n",
    "                                            nd.ones_like(sample_weight) * negative_weight)\n",
    "                    loss2 = l1_loss(score, tscore, score_weight)\n",
    "                    loss3 = l1_loss(xywh, tbox, sample_weight * box_weight)\n",
    "                    loss = loss1 + loss2 + loss3\n",
    "                loss.backward()\n",
    "                trainer.step(batch_size)\n",
    "                # update metrics\n",
    "                cls_loss.update(loss1)\n",
    "                obj_loss.update(loss2)\n",
    "                box_loss.update(loss3)\n",
    "\n",
    "                if global_step%100 == 0:\n",
    "                    sw.add_scalar(tag='cls_loss',value=cls_loss.get(), global_step=global_step)\n",
    "                    sw.add_scalar(tag='obj_loss',value=obj_loss.get(), global_step=global_step)\n",
    "                    sw.add_scalar(tag='box_loss',value=box_loss.get(), global_step=global_step)\n",
    "                    sw.add_scalar(tag='epoch', value=epoch, global_step=global_step)\n",
    "\n",
    "            now_lr = trainer.learning_rate\n",
    "            print('Epoch %2d, train %s %.5f, %s %.5f, %s %.5f time %.1f sec, learning rate %.15f' % (\n",
    "                epoch, *cls_loss.get(), *obj_loss.get(), *box_loss.get(), time.time()-tic, now_lr))\n",
    "            thefile.write('Epoch %2d, train %s %.5f, %s %.5f, %s %.5f time %.1f sec' % (epoch, *cls_loss.get(), *obj_loss.get(), *box_loss.get(), time.time()-tic))\n",
    "            thefile.close()\n",
    "            #net.export('/data/face_detector/logs_gluon/logs-'+unique_id+'/FaceJparams-'+unique_id+str(epoch))\n",
    "            sw.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"KeyboardInterrupted\")\n",
    "        sw.close()\n",
    "        exit(0)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters have to be consistent with FaceJ.py\n",
    "\n",
    "#resnet18 base model\n",
    "#net.collect_params().load('/data/face_detector/logs_gluon/logs-2019-01-15-14-13-37-669618/FaceJparams-2019-01-15-14-13-37-66961810-0000.params',ctx=ctx, allow_missing=False, ignore_extra=True)\n",
    "\n",
    "#resnet34 base model\n",
    "net.collect_params().load('/data/face_detector/logs_gluon/logs-2019-01-16-08-50-54-544639/FaceJparams-2019-01-16-08-50-54-544639295-0000.params',ctx=ctx, allow_missing=False, ignore_extra=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predic\n",
    "x, im = process_image('/data/face_detector/WIDER_test/9_Press_Conference_Press_Conference_9_783.jpg')\n",
    "out = predict(x.as_in_context(ctx))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predicted face with bounding box\n",
    "mpl.rcParams['figure.figsize'] = (8,8)\n",
    "colors = ['blue', 'green', 'red', 'black', 'magenta']\n",
    "display(im, out[0], threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().load('/data/face_detector/logs_gluon/logs-2019-01-16-08-50-54-544639/FaceJparams-2019-01-16-08-50-54-544639295-0000.params',ctx=ctx, allow_missing=False, ignore_extra=True)\n",
    "x, im = process_image('/data/face_detector/WIDER_test/9_Press_Conference_Press_Conference_9_783.jpg')\n",
    "out = predict(x.as_in_context(ctx))\n",
    "mpl.rcParams['figure.figsize'] = (8,8)\n",
    "colors = ['blue', 'green', 'red', 'black', 'magenta']\n",
    "display(im, out[0], threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
